{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook assumes you have already run the download routine (`s2s download -c <config_file.yaml>` on the terminal) and you desire to run custom processing on the downloaded segments\n",
    "\n",
    "## Table of contents\n",
    "\n",
    "- [Introduction](#introduction)\n",
    "- [Writing a custom processing routine with `imap`](#writing-a-custom-processing-routine-with-imap)\n",
    "- [Iterating over selected segments](#iterating-over-selected-segments)\n",
    "- [Accessing to the db (low level approach)](#accessing-the-db-low-level-approach)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Stream2segment has two main functions for working with downloaded data, **`process`** and **`imap`**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stream2segment.process import process, imap\n",
    "# you can type help(process) or help(imap) for documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both functions run custom code implemented in a so-called *processing function* iteratively on a selection of downloaded waveform segments: \n",
    "- `imap` yields the output of the processing function for each selected segment\n",
    "- `process` writes the output of the processing function for each selected segment in a row of a chosen tabular file (HDF or CSV)\n",
    "\n",
    "In this notebook, we will show how to work on downloaded segments via `imap`. An example of `process` is found in the Python module `paramtable.py` available in the same directory of this Notebook, if the latter has been created with the command `s2s init`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: The main goal of `paramtable.py` is to exemplify the case where a processing routine is run with a more \"low level\" approach from the terminal, without Jupyter: the module can be opened and modified easily following the documentation therein, and eventually run as script from the terminal:\n",
    "\n",
    "```console\n",
    "python paramtable.py\n",
    "```\n",
    "\n",
    "**In general, and especially for big data processing, this approach is the recommended way to process segments, as it avoids the unnecessary overhead of a Jupyter Notebook**, *which is designed for other purposes*. In addition, `imap` and `process` have several options that are not designed to be used within a Notebook (e.g., run with parallel sub-processes for faster execution, show progress bar with the % of task done and the estimated reamining time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing a custom processing routine with imap\n",
    "\n",
    "Regardless of your environment or design choice, a processing routine always involves the same operations: \n",
    "\n",
    "- Connect to a database\n",
    "- Select which segments to fetch\n",
    "- Run a processing function on the selected segments \n",
    "\n",
    "    \n",
    "### Database URL<a href=''>\n",
    "\n",
    "**DISCLAIMER: Pay attention when typing database URLs with passwords: do not commit or distribute them, try to avoid to type them on the terminal (or otherwise delete the command history)**\n",
    "\n",
    "In most cases, the database where the data has been downloaded and ready needs a simple string URL. For simplicity, a database URL is usually extracted from the download configuration file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "# uncomment the line below using an existing file on your OS:\n",
    "# with open('replace_with_your_download_config_path.yaml', 'r') as fpt:\n",
    "#    dburl = yaml.safe_load(fpt)['dburl']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this Notebook we will use the default example database (2 segments) available in the same directory of this file after executing the command `s2s init`. If you will have problem later accessing the database, check the current working directory and be sure the database is in it. A database URL can be written according to the [RFC-1738 syntax](https://docs.sqlalchemy.org/en/14/core/engines.html#database-urls):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "dbfile = os.path.join(os.getcwd(), 'example.db.sqlite')\n",
    "assert os.path.isfile(dbfile), \"Db file not found, please execute this notebook from within the db the directory\"\n",
    "dburl = 'sqlite:///' + dbfile\n",
    "# print(dburl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selection of segments\n",
    "\n",
    "The selection of suitable segments is performed by creating a `dict` mapping one or more Segment attributes to a selection expression for that attribute (for details on the segment object and its attributes, see [the segment object](https://github.com/rizac/stream2segment/wiki/the-segment-object))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the selection dict. This dict select a single segment (id=2) for illustrative purposes:\n",
    "segments_selection = {\n",
    "  'has_valid_data': 'true',\n",
    "  'maxgap_numsamples': '[-0.5, 0.5]',\n",
    "  'event_distance_deg': '[70, 80]'\n",
    "  # other optional attributes (see cheatsheet below for details):\n",
    "  # missing_data_sec: '<120'\n",
    "  # missing_data_ratio: '<0.5'\n",
    "  # id: '<300'\n",
    "  # event.time: \"(2014-01-01T00:00:00, 2014-12-31T23:59:59)\"\n",
    "  # event.latitude: \"[24, 70]\"\n",
    "  # event.longitude: \"[-11, 24]\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing function<a name='pyfunc'></a>\n",
    "\n",
    "A processing function has signature (arguments) `(segment, config)` where the first argument is a [segment object](https://github.com/rizac/stream2segment/wiki/the-segment-object) and the second is a custom dictionary of argument that can be passed to `imap` and `process` (in this example, we will not provide any config, thus the second argument will not be used).\n",
    "\n",
    "The function has no limitation on what can be implemented, you should only avoid to return the whole segment object as it might be detached (see next section for details), i.e. its attributes not accessible outside the processing function. However, you can safely return any of the segment attributes separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_processing_function(segment, config):\n",
    "    \"\"\"simple processing function. Take the segment stream and remove its instrumental response\"\"\"\n",
    "    # Get ObsPy Trace object. If the waveform has no gapos/overlaps, the trace is the only element\n",
    "    # of the segment stream object (otherwise the stream will have several traces):\n",
    "    trace = segment.stream()[0]\n",
    "    # remove the instrumental response of the Trace:\n",
    "    # get ObsPy Inventory object:\n",
    "    inventory = segment.inventory()\n",
    "    # remove the response:\n",
    "    trace_remresp = trace.remove_response(inventory)  # see caveat below\n",
    "    # return the segment.id, the event magnitude, the original trace and the trace with response removed\n",
    "    return segment.id, segment.event.magnitude, segment.stream()[0], trace_remresp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handling exceptions\n",
    "\n",
    "Any exception raised in the processing function (or any function called in it) will interrupt the whole processing routine with one special case: raising `stream2segment.process.SkipSegment` will cause the execution to resume from the next segment. \n",
    "\n",
    "`SkipSegment` is raised by default when the segment waveform data or inventory are malformed  (i.e., when `segment.stream()` and `segment.inventory()` fail), but can be used also to skip a segment programmatically, e.g.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stream2segment.process import SkipSegment\n",
    "\n",
    "def my_processing_function_raising(segment, config):\n",
    "    print('what')\n",
    "    if segment.sample_rate < 30: \n",
    "        raise SkipSegment(\"segment sample rate too low\")\n",
    "    # ... implement your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If a log file is given to `imap` or `process` (not shown here), then all segment skipped will be logged to file with their id and error message for later inspection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complete processing routine with  `imap`\n",
    "\n",
    "We can now illustrate a usage of, e.g., `imap`. In this case, the output of our processing function is simply printed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Segment Id: 2 (event magnitude: 8.1)\n",
      "Segment trace (first three points):\n",
      "  - Counts units (no response removed):    [-1.42699395e-06 -1.43603990e-06 -1.42210201e-06]\n",
      "  - Physical units (response removed):     [-1.42699395e-06 -1.43603990e-06 -1.42210201e-06]\n"
     ]
    }
   ],
   "source": [
    "from stream2segment.process import imap\n",
    "\n",
    "for (segment_id, mag, trace, trace_remresp) in imap(my_processing_function, dburl, segments_selection):  \n",
    "    print()\n",
    "    print('Segment Id: %d (event magnitude: %.1f)' % (segment_id, mag))\n",
    "    print('Segment trace (first three points):')\n",
    "    print('  - Counts units (no response removed):    %s' % trace.data[:3])\n",
    "    print('  - Physical units (response removed):     %s' % trace_remresp.data[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Caveat</b>: As you can see, the returned trace expected in count units is actually the same as the trace with response removed. This happened because **many `Stream` and `Trace` methods** (check ObsPy documentation in case of doubts) modify the objects in-place, i.e. they **permanently modify the returned value of `segment.stream()`** (which is cached in the segment object for performance reasons).\n",
    "\n",
    "A solution is to call `segment.stream(reload=True)` that forces the complete reload of the stream from the database, or simply work on copies of those objects (which is generally faster), e.g.: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Segment Id: 2 (event magnitude: 8.1)\n",
      "Segment trace (first three points):\n",
      "  - Counts units (no response removed):    [-314 -280 -251]\n",
      "  - Physical units (response removed):     [-1.42699395e-06 -1.43603990e-06 -1.42210201e-06]\n"
     ]
    }
   ],
   "source": [
    "def my_processing_function(segment, config):\n",
    "    \"\"\"simple processing function. Take the segment stream and remove its instrumental response\"\"\"\n",
    "    # Get ObsPy Trace object and make a COPY of IT:\n",
    "    trace = segment.stream()[0].copy()\n",
    "\n",
    "    # now, segment.stream()[0] and trace are two different objects.\n",
    "    # By just running the same code as in the previous processing function\n",
    "    # we will remove the response to `trace` and preserve `segment.stream()`\n",
    "    # (but you can do also the other way around, depending on your needs)\n",
    "    inventory = segment.inventory()\n",
    "    trace_remresp = trace.remove_response(inventory)  # see caveat below\n",
    "    return segment.id, segment.event.magnitude, segment.stream()[0], trace_remresp\n",
    "\n",
    "# And now we get the expected result:\n",
    "for (segment_id, mag, trace, trace_remresp) in imap(my_processing_function, dburl, segments_selection):\n",
    "    print()\n",
    "    print('Segment Id: %d (event magnitude: %.1f)' % (segment_id, mag))\n",
    "    print('Segment trace (first three points):')\n",
    "    print('  - Counts units (no response removed):    %s' % trace.data[:3])\n",
    "    print('  - Physical units (response removed):     %s' % trace_remresp.data[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterating over selected segments<a name='iterating_over_selected_segments'></a>\n",
    "\n",
    "Although builtin functions `imap` and `process` should fit most of the user needs, if you need even more customization, a more low-level approach consists of simply work iteratively on the selected segments via `get_segments`. This is basically what the two builtin functions do under the hood with a given segments selection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stream2segment.process import get_segments\n",
    "\n",
    "for seg in get_segments(dburl, segments_selection):\n",
    "    # do your work here... In this case, with the first segment `seg` just loaded, simply exit the loop:\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`get_segments` (and consequently, `imap` and `process` using it) opens a database session, yields selected segments and closes the session afterwards. A [database session](https://docs.sqlalchemy.org/en/latest/orm/session_basics.html) is an object that establishes all conversations with the database and represents a \"holding zone\"  for all the objects which you’ve loaded or associated with it during its lifespan.\n",
    "\n",
    "Closing a session is recommended after you finished your work as it releases memory on the computer and (if the db is remote) on the server, avoiding potential problems. `get_segments`, `imap` and `process` do this automatically. Note that after a session is closed, all segment objects are **detached** from the database, which means we can not access anymore all of its properties, but only those previously loaded. E.g., accessing the segment related objects (e.g. the event object) outside the for loop, raises an error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR: Parent instance <Segment at 0x1303d7460> is not bound to a Session; lazy load operation of attribute 'event' cannot proceed (Background on this error at: http://sqlalche.me/e/13/bhk3)\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    seg.event\n",
    "except Exception as exc:\n",
    "    print('ERROR: ' + str(exc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accessing to the db (low level approach)<a name='accessing-the-db-low-level-approach'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In very specific cases where you want to keep the segments and all related objects accessible (i.e. attached to a session) also outside a `get_segments` for-loop, you can then call `get_segments` with a session object instead of a db url. \n",
    "\n",
    "Using the `session` object is in general for users experienced with the [DB library](https://docs.sqlalchemy.org/en/20/orm/quickstart.html) employed by stream2segment.\n",
    "\n",
    "For these cases, we will show hereafter some properties of the `Segment` object in more details "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stream2segment.process import get_session, Segment\n",
    "\n",
    "session = get_session(dburl)\n",
    "\n",
    "# query a random segment with a dummy filter just for illustrative purposes:\n",
    "seg = session.query(Segment).filter(Segment.id < 10).first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backreferences (on related object's)\n",
    "\n",
    "With the session still open, we can access some of the segments related objects (this will issue a query to the database):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Event\n",
      " attributes (16 of 16 loaded):\n",
      "  event_id: 20170908_0 (str, 16 characters, showing first 10 only)\n",
      "  time: 2017-09-08 04:49:21.200000 (datetime)\n",
      "  latitude: 15.02 (float)\n",
      "  longitude: -93.81 (float)\n",
      "  depth_km: 72.0 (float)\n",
      "  author: EMSC (str)\n",
      "  catalog: EMSC-RTS (str)\n",
      "  contributor: EMSC (str)\n",
      "  contributor_id: 616600 (str)\n",
      "  mag_type: mw (str)\n",
      "  magnitude: 8.1 (float)\n",
      "  mag_author: EMSC (str)\n",
      "  event_location_name: OFFSHORE C (str, 24 characters, showing first 10 only)\n",
      "  event_type: None (NoneType)\n",
      "  webservice_id: 1 (int)\n",
      "  id: 1 (int)\n",
      " related_objects (0 of 2 loaded):\n",
      "  webservice\n",
      "  segments\n",
      "Station\n",
      " attributes (11 of 11 loaded):\n",
      "  network: GE (str)\n",
      "  station: RUE (str)\n",
      "  latitude: 52.4759 (float)\n",
      "  longitude: 13.78 (float)\n",
      "  elevation: 40.0 (float)\n",
      "  site_name: None (NoneType)\n",
      "  start_time: 2012-03-21 10:00:00 (datetime)\n",
      "  end_time: None (NoneType)\n",
      "  stationxml: b'\\x1f\\x8b\\x08\\x00\\xa4\\x99\\x1b\\\\\\x02\\xff' (bytes, 44710 elements, showing first 10 only)\n",
      "  datacenter_id: 1 (int)\n",
      "  id: 2 (int)\n",
      " related_objects (0 of 3 loaded):\n",
      "  datacenter\n",
      "  channels\n",
      "  segments\n"
     ]
    }
   ],
   "source": [
    "evt = seg.event\n",
    "print(str(evt))\n",
    "\n",
    "sta = seg.station\n",
    "print(str(sta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that both objects have, among their related objects, a so-called back-reference `segments`. This is a list-like object of `Segment`s (among which the original `seg` object) and not a single entity because of a so-called \"many-to-one\" relationship: one segment is always related to one event/station, whereas one event generates many segments at different station, and one station generates many segments for different events.\n",
    "\n",
    "This extremely powerful feature connecting several entities effortless is a natural consequence of being backed by  a relational database and it would be nearly impossible to get with a classical file system storage. Neverthless, it should be used with care with massive downloads, as one might load in the `session` huge amount of data, causing memory overflows. A solution might be to call from times to times `session.expunge_all()` which remove all object instances from the session (possibly freeing memory) or load only each object id, deferring the load of all other attributes from the database upon access, e.g.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy.orm import load_only\n",
    "\n",
    "evt = seg.event\n",
    "# load event related segments (*risk of memory overflow: low):\n",
    "segments = evt.segments.options(load_only(Segment.id)).all()\n",
    "\n",
    "cha = seg.channel\n",
    "# load channel related segments (*risk of memory overflow: medium):\n",
    "segments = cha.segments.options(load_only(Segment.id)).all()\n",
    "\n",
    "sta = seg.station\n",
    "# load station related segments (*risk of memory overflow: high):\n",
    "segments = sta.segments.options(load_only(Segment.id)).all()\n",
    "\n",
    "dct = seg.datacenter\n",
    "# load data center (e.g. IRIS) related segments (*risk of memory overflow: very high):\n",
    "segments = dct.segments.options(load_only(Segment.id)).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\* The levels of risk reported are just heuristically estimated and have to be considered reliable only relative to each other (an event has almost certainly less related segments than a channel, which has almost certainly less related segments than a station, and so on)\n",
    "\n",
    "**In any case, for really memory consuming or long tasks, as already recommended, consider moving the Notebook code into a custom Python module and execute the module as a script or via use the command `s2s process`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# finally close session\n",
    "from stream2segment.process import close_session\n",
    "close_session(session)\n",
    "# If close_session returns True, the session and the database connection are now closed.\n",
    "# If you want to close the session only (freeing memory but keeping the session reusable later):\n",
    "close_session(session, dispose_engine=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
